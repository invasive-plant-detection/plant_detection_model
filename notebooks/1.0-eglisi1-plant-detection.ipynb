{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic plant detection for starting Bachelor thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data PlantCLEF2022_trusted_training_metadata.csv form /data/01_raw/PlantCLEF2022_trusted_training_metadata.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"../data/02_processed/merged_data.csv\"\n",
    "try:\n",
    "    data = pd.read_csv(file_path, delimiter=\";\")\n",
    "    if data.empty:\n",
    "        print(\"The CSV file is empty\")\n",
    "    else:\n",
    "        print(data.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"File not found: {file_path}. Please check the file path and that the file is downloaded.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete already existing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "temp_dir = \"temp\"\n",
    "\n",
    "# delete and create tmp dir to ensure it's empty\n",
    "os.system(f\"rm -rf {temp_dir}\")\n",
    "\n",
    "# Ensure the tmp/ directory exists\n",
    "os.makedirs(temp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download data per plant parallelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def download_image(index_url):\n",
    "    index, url = index_url\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        file_path = os.path.join(temp_dir, f\"{index}.jpg\")\n",
    "\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        # print(f\"Downloaded {url} to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "\n",
    "# Prepare a list of tuples containing the index and URL for each image\n",
    "index_url_list = [(index, row[\"image_backup_url\"]) for index, row in data.iterrows()]\n",
    "\n",
    "# Use ThreadPoolExecutor to download images in parallel\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    executor.map(download_image, index_url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add image path to metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"image_path\"] = [f\"{temp_dir}/{index}.jpg\" for index in data.index]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target_size = (224, 224) # todo: is now random choose a good size afterwards\n",
    "\n",
    "# Function to load images\n",
    "def load_image(image_path: str) -> np.ndarray:\n",
    "    image = Image.open(image_path).convert('RGB')  # Ensure RGB format\n",
    "    image = image.resize(target_size)\n",
    "    image_array = np.asarray(image)\n",
    "    return image_array\n",
    "\n",
    "# Load images into arrays\n",
    "images = np.array([load_image(path) for path in data['image_path']])\n",
    "\n",
    "# Encode labels \n",
    "# transform into numerical values to train classifier\n",
    "encoder = LabelEncoder()\n",
    "data['encoded_labels'] = encoder.fit_transform(data['species'])  \n",
    "labels = data['encoded_labels'].values\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    images, labels, test_size=0.2, random_state=42  # Adjust test_size as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "classes = {label: class_name for label, class_name in enumerate(encoder.classes_)}\n",
    "with open('classes.json', 'w') as f:\n",
    "    json.dump(classes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = (224, 224, 3)),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_initializer = tf.keras.initializers.he_normal),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer = tf.keras.regularizers.L2(0.01)),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(num_classes + 1, activation = 'softmax') # todo: +1 makes no sense?\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thoroughly examine y_train\n",
    "print(\"Unique labels: \", np.unique(y_train))\n",
    "print(\"Min label: \", np.min(y_train))\n",
    "print(\"Max label: \", np.max(y_train))\n",
    "print(\"Full y_train array:\\n\", y_train) \n",
    "\n",
    "# If there's an unexpected value, investigate further with:\n",
    "if np.max(y_train) >= 62: \n",
    "    for i, label in enumerate(y_train):\n",
    "        if label >= 62:\n",
    "            print(f\"Invalid label {label} found at index {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data\n",
    "mapping = {}\n",
    "\n",
    "for species, label in zip(df['species'], df['encoded_labels']):\n",
    "    if species not in mapping:\n",
    "        mapping[species] = label\n",
    "\n",
    "for species, label in mapping.items():\n",
    "    print(f\"{species}: {label}\")\n",
    "\n",
    "# Method 2: Using pandas.DataFrame.drop_duplicates \n",
    "unique_df = df[['species', 'encoded_labels']].drop_duplicates()\n",
    "\n",
    "for index, row in unique_df.iterrows():\n",
    "    print(f\"{row['species']}: {row['encoded_labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/my_model\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "save_model = True\n",
    "\n",
    "if save_model:\n",
    "    now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    model.save(f\"../models/model-{now}.keras\")\n",
    "else:\n",
    "    print(\"Model not saved. Set save_model to True to save the model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
