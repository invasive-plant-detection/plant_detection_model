{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic plant detection for starting Bachelor thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   classid                                         image_path  \\\n",
      "0  5328909  5328909/de73353bbf8431ec594df8c0c070fa5d562756...   \n",
      "1  5328909  5328909/cb1b1aac1895f8f5a52e1c85ef8ceae7580e68...   \n",
      "2  5328909  5328909/4bdb06e3f9b4b61c9ed2a269498d064b41b1d0...   \n",
      "3  5328909  5328909/2e5095764b764e63fca8150bab3aa1bbc2157a...   \n",
      "4  5328909  5328909/9886a43a74aa9c493667235bb98786df965706...   \n",
      "\n",
      "                       species       genus        family        order  \\\n",
      "0  sagittaria latifolia willd.  Sagittaria  Alismataceae  Alismatales   \n",
      "1  sagittaria latifolia willd.  Sagittaria  Alismataceae  Alismatales   \n",
      "2  sagittaria latifolia willd.  Sagittaria  Alismataceae  Alismatales   \n",
      "3  sagittaria latifolia willd.  Sagittaria  Alismataceae  Alismatales   \n",
      "4  sagittaria latifolia willd.  Sagittaria  Alismataceae  Alismatales   \n",
      "\n",
      "        class                                   image_backup_url  \n",
      "0  Liliopsida  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n",
      "1  Liliopsida  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n",
      "2  Liliopsida  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n",
      "3  Liliopsida  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n",
      "4  Liliopsida  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n"
     ]
    }
   ],
   "source": [
    "# load data PlantCLEF2022_trusted_training_metadata.csv form /data/01_raw/PlantCLEF2022_trusted_training_metadata.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"../data/02_processed/merged_data.csv\"\n",
    "try:\n",
    "    data = pd.read_csv(file_path, delimiter=\";\")\n",
    "    if data.empty:\n",
    "        print(\"The CSV file is empty\")\n",
    "    else:\n",
    "        print(data.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"File not found: {file_path}. Please check the file path and that the file is downloaded.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use the 5 most occuring species "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the 5 most occuring species in the dataset and remove the rest\n",
    "\n",
    "AMOUNT_OF_SPECIES = 5\n",
    "\n",
    "species = data[\"species\"].value_counts().head(AMOUNT_OF_SPECIES)\n",
    "species = species.index.tolist()\n",
    "data = data[data[\"species\"].isin(species)]\n",
    "\n",
    "species = data[\"species\"].unique()\n",
    "len(species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete already existing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "temp_dir = \"temp\"\n",
    "\n",
    "# delete and create tmp dir to ensure it's empty\n",
    "os.system(f\"rm -rf {temp_dir}\")\n",
    "\n",
    "# Ensure the tmp/ directory exists\n",
    "os.makedirs(temp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download data per plant parallelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def download_image(index_url: str) -> None:\n",
    "    index, url = index_url\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        file_path = os.path.join(temp_dir, f\"{index}.jpg\")\n",
    "\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        # print(f\"Downloaded {url} to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "\n",
    "# Prepare a list of tuples containing the index and URL for each image\n",
    "index_url_list = [(index, row[\"image_backup_url\"]) for index, row in data.iterrows()]\n",
    "\n",
    "# Use ThreadPoolExecutor to download images in parallel\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    executor.map(download_image, index_url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add image path to metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classid</th>\n",
       "      <th>image_path</th>\n",
       "      <th>species</th>\n",
       "      <th>genus</th>\n",
       "      <th>family</th>\n",
       "      <th>order</th>\n",
       "      <th>class</th>\n",
       "      <th>image_backup_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>8002952</td>\n",
       "      <td>temp/580.jpg</td>\n",
       "      <td>ambrosia artemisiifolia l.</td>\n",
       "      <td>Ambrosia</td>\n",
       "      <td>Asteraceae</td>\n",
       "      <td>Asterales</td>\n",
       "      <td>Magnoliopsida</td>\n",
       "      <td>https://lab.plantnet.org/LifeCLEF/PlantCLEF202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>8002952</td>\n",
       "      <td>temp/581.jpg</td>\n",
       "      <td>ambrosia artemisiifolia l.</td>\n",
       "      <td>Ambrosia</td>\n",
       "      <td>Asteraceae</td>\n",
       "      <td>Asterales</td>\n",
       "      <td>Magnoliopsida</td>\n",
       "      <td>https://lab.plantnet.org/LifeCLEF/PlantCLEF202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>8002952</td>\n",
       "      <td>temp/582.jpg</td>\n",
       "      <td>ambrosia artemisiifolia l.</td>\n",
       "      <td>Ambrosia</td>\n",
       "      <td>Asteraceae</td>\n",
       "      <td>Asterales</td>\n",
       "      <td>Magnoliopsida</td>\n",
       "      <td>https://lab.plantnet.org/LifeCLEF/PlantCLEF202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>8002952</td>\n",
       "      <td>temp/583.jpg</td>\n",
       "      <td>ambrosia artemisiifolia l.</td>\n",
       "      <td>Ambrosia</td>\n",
       "      <td>Asteraceae</td>\n",
       "      <td>Asterales</td>\n",
       "      <td>Magnoliopsida</td>\n",
       "      <td>https://lab.plantnet.org/LifeCLEF/PlantCLEF202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>8002952</td>\n",
       "      <td>temp/584.jpg</td>\n",
       "      <td>ambrosia artemisiifolia l.</td>\n",
       "      <td>Ambrosia</td>\n",
       "      <td>Asteraceae</td>\n",
       "      <td>Asterales</td>\n",
       "      <td>Magnoliopsida</td>\n",
       "      <td>https://lab.plantnet.org/LifeCLEF/PlantCLEF202...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     classid    image_path                     species     genus      family  \\\n",
       "580  8002952  temp/580.jpg  ambrosia artemisiifolia l.  Ambrosia  Asteraceae   \n",
       "581  8002952  temp/581.jpg  ambrosia artemisiifolia l.  Ambrosia  Asteraceae   \n",
       "582  8002952  temp/582.jpg  ambrosia artemisiifolia l.  Ambrosia  Asteraceae   \n",
       "583  8002952  temp/583.jpg  ambrosia artemisiifolia l.  Ambrosia  Asteraceae   \n",
       "584  8002952  temp/584.jpg  ambrosia artemisiifolia l.  Ambrosia  Asteraceae   \n",
       "\n",
       "         order          class  \\\n",
       "580  Asterales  Magnoliopsida   \n",
       "581  Asterales  Magnoliopsida   \n",
       "582  Asterales  Magnoliopsida   \n",
       "583  Asterales  Magnoliopsida   \n",
       "584  Asterales  Magnoliopsida   \n",
       "\n",
       "                                      image_backup_url  \n",
       "580  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n",
       "581  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n",
       "582  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n",
       "583  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n",
       "584  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"image_path\"] = [f\"{temp_dir}/{index}.jpg\" for index in data.index]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "IMAGE_SIZE = [224, 224]\n",
    "BATCH_SIZE = 32\n",
    "CHANNELS = 3\n",
    "\n",
    "train_df = data.sample(frac=0.8, random_state=123)  # 80% for training\n",
    "val_df = data.drop(train_df.index)                  # Remaining 20% for validation\n",
    "\n",
    "\n",
    "# Fit the label encoder and transform the 'classid' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['species_encoded'] = label_encoder.fit_transform(data['species'])\n",
    "\n",
    "# Create a mapping from encoded labels back to the original class names\n",
    "classes = {str(index): name for index, name in enumerate(label_encoder.classes_)}\n",
    "with open('classes.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(classes, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def preprocess_image(image_path: str, label: str) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = image / 255.0  # Normalize pixel values\n",
    "    return image, label\n",
    "\n",
    "def df_to_dataset(dataframe: pd.DataFrame, shuffle=True, batch_size=BATCH_SIZE) -> tf.data.Dataset:\n",
    "    images = dataframe['image_path'].values\n",
    "    labels = dataframe['classid_encoded'].values\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    ds = ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_dataset = df_to_dataset(train_df, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_dataset = df_to_dataset(val_df, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import datetime\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "AMOUNT_EPOCHS = 10\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_names, input_shape, num_classes, train_dataset, val_dataset, batch_size):\n",
    "        self.model_names = model_names\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        module = importlib.import_module('tensorflow.keras.applications')\n",
    "        model_class = getattr(module, model_name)\n",
    "        base_model = model_class(weights='imagenet', include_top=False, input_shape=self.input_shape)\n",
    "        base_model.trainable = False\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(1024, activation='relu')(x)  # Dense layer for feature interpretation\n",
    "        predictions = Dense(self.num_classes, activation='softmax')(x)\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        return model\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        results = []\n",
    "        for model_name in self.model_names:\n",
    "            print(f\"Training and evaluating {model_name}\")\n",
    "            model = self.load_model(model_name)\n",
    "            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            history = model.fit(\n",
    "                self.train_dataset,\n",
    "                steps_per_epoch=len(train_df) // self.batch_size,\n",
    "                epochs=AMOUNT_EPOCHS,\n",
    "                validation_data=self.val_dataset,\n",
    "                validation_steps=len(val_df) // self.batch_size\n",
    "            )\n",
    "            val_loss, val_accuracy = model.evaluate(self.val_dataset, steps=len(val_df) // self.batch_size)\n",
    "            print(f\"{model_name} - Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n",
    "            model_filename = f\"{model_name}-{val_accuracy:.4f}-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}.keras\"\n",
    "            model.save(f'../models/{model_filename}')\n",
    "            results.append((model_name, val_accuracy, model_filename))\n",
    "            print('---------------------------')\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['MobileNetV2', 'MobileNetV3Large', 'MobileNetV3Small', 'EfficientNetV2S'] \n",
    "manager = ModelManager(\n",
    "    model_names=model_names,\n",
    "    input_shape=(*IMAGE_SIZE, CHANNELS),\n",
    "    num_classes=AMOUNT_OF_SPECIES,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "results = manager.train_and_evaluate()\n",
    "for result in results:\n",
    "    print(f\"Model: {result[0]}, Accuracy: {result[1]:.4f}, Saved as: {result[2]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
